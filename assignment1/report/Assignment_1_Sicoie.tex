
\documentclass[11pt]{article}
\usepackage[letterpaper, margin=1in]{geometry}

% Used to place appendix in the toc with proper titles
\usepackage[toc,page,title]{appendix}
\renewcommand{\appendixname}{Appendix} % Used to rename from "Appendices" to "Appendix"
\renewcommand{\appendixpagename}{Appendix} % Used to rename from "Appendices" to "Appendix"
\renewcommand{\appendixtocname}{Appendix} % Used to rename from "Appendices" to "Appendix"

% Used to make citations/bibliography
% Usage:
% \bibliography{./bibliography/fullbib.bib}
% \bibliographystyle{plain}
\usepackage{cite}

% To enable forcing figure placement
\usepackage{float}

% Used to skip to next paragraph when a blank line is used between sentences
\usepackage{parskip}

% Used to change line spacing (double spaced, 1.5, etc.)
% Usage: 
% \begin{spacing}{1.5}
% \end{spacing}
\usepackage{setspace}

% Used for inserting images.
% Usage: 
% \begin{figure}[hpt]
%     \centering
%     \caption{Caption}
%     \includegraphics[width=0.6\textwidth]{images/<IMG.NAME>}
%     \label{fig:<LABEL>}
% \end{figure}
\usepackage{graphicx}
\graphicspath{ {./images/} }

% Subplots
\usepackage{subcaption}

% Used for in text code with formatting and language highlighting
\usepackage{listings}
\usepackage{xcolor}

\usepackage[breaklinks]{hyperref}
\usepackage{url}

% Define custom style
\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily\footnotesize, % Use a monospaced font with strict sizing
    numbers=left, % Show line numbers
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    stepnumber=1, % Ensures every line gets numbered
    tabsize=4, % Set tab width
    keepspaces=true, % Preserve spaces
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    breakatwhitespace=false,
    columns=fixed, % Maintain strict spacing
    lineskip=-1pt, % Prevent additional spacing between lines
    frame=single, % Optional: adds a border around the code
    rulecolor=\color{black},
    backgroundcolor=\color{white},
    keywordstyle=\color{blue}\bfseries, % Style for keywords
    stringstyle=\color{red}, % Style for strings
    commentstyle=\color{gray}, % Style for comments
    xleftmargin=5pt, % Adjusts left margin outside the frame
    xrightmargin=5pt % Adjusts right margin outside the frame
}
\renewcommand{\lstlistingname}{Code Block}


\lstset{style=mystyle}

% Used to create hyperlink references that can be clicked within the PDF
% Usage: \ref{<SECTIONLABEL>}
\usepackage{hyperref}

% Used for certain math symbols
\usepackage{amssymb}

\usepackage{array}
\usepackage{ragged2e}

\usepackage{titlesec}
\usepackage{bookmark}

\setlength\parindent{0pt}

\title{%
  ELEC 873 \\
Assignment 1}

\author{Anthony Sicoie (20214793)}

\begin{document}

\maketitle


\section*{Q1:}

\subsection*{Code:}

Code Blocks \ref{q1:mpi}, \ref{q1:openmp}, and \ref{q1:pthread} present the question 1 code using MPI, OpenMP, and Pthreads respectively to print basic "Hello World" messages.
The output for the code can be found in the Log Sub Section in Code Blocks \ref{q1:mpilog}, \ref{q1:openmplog}, and \ref{q1:pthreadlog}.

\lstinputlisting[language=C, caption=\centering{MPI code for question 1}, label={q1:mpi}]{../q1/mpi.c}
\lstinputlisting[language=C, caption=\centering{OpenMP code for question 1}, label={q1:openmp}]{../q1/openmp.c}
\lstinputlisting[language=C, caption=\centering{Pthread code for question 1}, label={q1:pthread}]{../q1/pthread.c}

\subsection*{Logs:}
All code was run with 5 processes/threads, though any number can be set via the \texttt{Makefile} \texttt{make runall n=<number\_of\_threads/processes>} directive. 
This can be seen in the Code Block \ref{a:make} of the appendix where the makefile code is provided.
This is the \texttt{Makefile} template that all other questions also followed, with appropriate changes based on the code requirements for each one.

\lstinputlisting[language=sh, caption=\centering{MPI output for question 1}, label={q1:mpilog}]{../q1/mpi.log}
\lstinputlisting[language=sh, caption=\centering{OpenMP output for question 1}, label={q1:openmplog}]{../q1/openmp.log}
\lstinputlisting[language=sh, caption=\centering{Pthread output for question 1}, label={q1:pthreadlog}]{../q1/pthread.log}

\section*{Q2:}

\subsection*{Code:}
Code Blocks \ref{q2:mpi}, \ref{q2:openmp}, and \ref{q2:pthread} present the question 2 code using MPI, OpenMP, and Pthreads respectively to compute \textit{pi}.
The output for the code can be found in the Log Sub Section in Code Blocks \ref{q2:mpilog}, \ref{q2:openmplog}, and \ref{q2:pthreadlog}.

\lstinputlisting[language=C, caption=\centering{MPI code for question 2}, label={q2:mpi}]{../q2/mpi.c}
\lstinputlisting[language=C, caption=\centering{OpenMP code for question 2}, label={q2:openmp}]{../q2/openmp.c}
\lstinputlisting[language=C, caption=\centering{Pthread code for question 2}, label={q2:pthread}]{../q2/pthread.c}

\subsection*{Logs:}
\lstinputlisting[language=sh, caption=\centering{MPI output for question 2}, label={q2:mpilog}]{../q2/mpi.log}
\lstinputlisting[language=sh, caption=\centering{OpenMP output for question 2}, label={q2:openmplog}]{../q2/openmp.log}
\lstinputlisting[language=sh, caption=\centering{Pthread output for question 2}, label={q2:pthreadlog}]{../q2/pthread.log}

\section*{Q3:}
\subsection*{Code:}
Code Block \ref{q3:openmp} presents the question 3 code using OpenMP to compute the sum of 1000 numbers.
The output for the code can be found in the Log Sub Section in Code Block \ref{q3:openmplog}.

\lstinputlisting[language=C, caption=\centering{OpenMP code for question 3}, label={q3:openmp}]{../q3/openmp.c}

\subsection*{Logs:}
\lstinputlisting[language=sh, caption=\centering{OpenMP output for question 3}, label={q3:openmplog}]{../q3/openmp.log}

\section*{Q4:}
\subsection*{Code:}
Code Block \ref{q4:mpi} presents the question 4 code using MPI to perform send operations in a unidirectional ring of 8 processes.
The output for the code can be found in the Log Sub Section in Code Block \ref{q4:mpilog}, however it is worth noting that the code is modular and can function with more or less than 8 processes.

\lstinputlisting[language=C, caption=\centering{MPI code for question 4}, label={q4:mpi}]{../q4/mpi.c}

\subsection*{Logs:}
\lstinputlisting[language=sh, caption=\centering{MPI output for question 4}, label={q4:mpilog}]{../q4/mpi.log}

\section*{Q5:}

\subsection*{Code:}
Code Block \ref{q5:mpi} presents the question 5 code using MPI to represent a 3D Mesh.
The output for the code can be found in the Log Sub Section in Code Block \ref{q5:mpilog}, however it is worth noting that the code is modular and can function with more than 8 processes so long as they are cubes.

\lstinputlisting[language=C, caption=\centering{MPI code for question 5}, label={q5:mpi}]{../q5/mpi.c}

\subsection*{Logs:}
\lstinputlisting[language=sh, caption=\centering{MPI output for question 5}, label={q5:mpilog}]{../q5/mpi.log}

\section*{Q6:}

\subsection*{Code:}
Code Block \ref{q6:mpi} presents the question 6 code using MPI to do a ring-based scatter leveraging \texttt{MPI\_Pack} and \texttt{MPI\_Unpack}.
The output for the code can be found in the Log Sub Section in Code Block \ref{q6:mpilog}.

\lstinputlisting[language=C, caption=\centering{MPI code for question 6}, label={q6:mpi}]{../q6/mpi.c}

\subsection*{Logs:}
\lstinputlisting[language=sh, caption=\centering{MPI output for question 6}, label={q6:mpilog}]{../q6/mpi.log}

\section*{Q7:}

\subsection*{Code:}
Code Blocks \ref{q7:mpi}, \ref{q7:openmp}, \ref{q7:pthread}, and \ref{q7:hybrid} present the question 7 code using MPI, OpenMP, Pthreads, and a Hybrid MPI+OpenMP implementation respectively to find the min in an array.
Of note is the hybrid code, which allows the user to vary the array size via a command line argument and also handles timing as explained in the Analysis Sub Section.
The output for the code can be found in the Log Sub Section in Code Blocks \ref{q7:mpilog}, \ref{q7:openmplog}, and \ref{q7:pthreadlog} with the exception of the hybrid model, which for the sake of brevity is omitted.

\lstinputlisting[language=C, caption=\centering{MPI code for question 7}, label={q7:mpi}]{../q7/mpi.c}
\lstinputlisting[language=C, caption=\centering{OpenMP code for question 7}, label={q7:openmp}]{../q7/openmp.c}
\lstinputlisting[language=C, caption=\centering{Pthread code for question 7}, label={q7:pthread}]{../q7/pthread.c}
\lstinputlisting[language=C, caption=\centering{Hybrid (MPI+OpenMP) code for question 7}, label={q7:hybrid}]{../q7/hybrid.c}

\subsection*{Analysis:}

\begin{figure*}
\centering
\begin{subfigure}[t]{0.48\textwidth}
  \centering
    \includegraphics[width=\textwidth]{./images/knlq7.png}
  \caption{Varied process$\times$thread configurations run on the KNL server}\label{fig:knlq7}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{0.48\textwidth}
  \centering
    \includegraphics[width=\textwidth]{./images/localq7.png}
  \caption{Varied process$\times$thread configurations run locally}\label{fig:localq7}
\end{subfigure}%
\caption{Figures depicting the duration in seconds associated with varying process$\times$thread configurations on KNL server and local system}
\label{fig:q7}
\end{figure*}

Figures \ref{fig:knlq7} and \ref{fig:localq7} show the performance of numerous process and thread configurations (2 processes with 8 threads, 4 processes with 4 threads, and 8 processes with 2 threads) on the KNL system as well as my local system.
The figures compare the performance as the size of the array scales exponentially following a log base 2 pattern, with the smallest size being $2^8$ and the largest being $2^{15}$.
The tests were run via the \texttt{run.sh} script present in the Appendix in Code Block \ref{a:runq7}.
The script varied the array sizes and the process/thread configuration.
The code presented in Code Block \ref{q7:hybrid} iterated through the loop 100 times and retrieved the average time via \texttt{MPI\_Wtime()}; the timer overhead was also deducted accordingly.
As is expected, as the array size increases, all iterations take longer.
I extended this simulation locally to see how it would perform as my machine is capable of producing results quicker, and once the array size reaches anything beyond $2^{17}$, the differences are largely indistinguishable.
The plot of that data can be seen in Figure \ref{fig:bigq7}.
Despite this, these plots do point towards the fact that the implementations favouring MPI processes (8 processes by 2 threads and 4 processes by 4 threads) are more efficient at smaller array sizes as can be seen between data points $x=2^8$ and $x=2^{12}$.
This may be due to the fact that the OpenMP directive incurs additional overhead when compared to the associated MPI overhead.

\begin{figure*}
\centering
    \includegraphics[width=\textwidth]{./images/bigq7.png}
\caption{Graph depicting the duration in seconds associated with varying process$\times$thread configurations on my local machine}
\label{fig:bigq7}
\end{figure*}

\subsection*{Logs:}
\lstinputlisting[language=sh, caption=\centering{MPI output for question 7}, label={q7:mpilog}]{../q7/mpi.log}
\lstinputlisting[language=sh, caption=\centering{OpenMP output for question 7}, label={q7:openmplog}]{../q7/openmp.log}
\lstinputlisting[language=sh, caption=\centering{Pthread output for question 7}, label={q7:pthreadlog}]{../q7/pthread.log}
% \lstinputlisting[language=sh, caption=\centering{Hybrid (MPI+OpenMP) output for question 7}, label={q7:hybridlog}]{../q7/hybrid.log}

\section*{Q8:}

\subsection*{Code:}
Code Blocks \ref{q8:mpiS} and \ref{q8:mpiD} present the question 8 code using MPI to calculate elements within the Mandelbrot set via static and dynamic scheduling respectively.
The output of the code can be seen in Figures \ref{fig:q8S} and \ref{fig:q8D}, with the code that generated the coloured images themselves in Code Block \ref{q8:mpidisp}.
Analysis of the performance of the code is in the Sub Section Analysis.
\lstinputlisting[language=C, caption=\centering{MPI code for question 8 static assignment}, label={q8:mpiS}]{../q8/mpi_static.c}
\lstinputlisting[language=C, caption=\centering{MPI code for question 8 dynamic assignment}, label={q8:mpiD}]{../q8/mpi_dynamic.c}
\lstinputlisting[language=C, caption=\centering{MPI code for question 8 dynamic assignment}, firstline=110, lastline=125, label={q8:mpidisp}]{../q8/mandelbrot-display.c}


\begin{figure*}
\centering
\begin{subfigure}[t]{0.48\textwidth}
  \centering
    \includegraphics[width=\textwidth]{./images/static.png}
  \caption{static mandelbrot set coloured output}\label{fig:q8s}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{0.48\textwidth}
  \centering
    \includegraphics[width=\textwidth]{./images/dynamic.png}
  \caption{dynamic mandelbrot set coloured output}\label{fig:q8d}
\end{subfigure}%
\caption{figures depicting the coloured mandelbrot set at 800$\times$800 resolution}
\label{fig:outq8}
\end{figure*}

\subsection*{Analysis:}

Similar to question 7, the code was run both on the KNL system and on my own local machine.
The graphs visible in Figure \ref{fig:q8S} and \ref{fig:q8D} were generated using the scripts in the Appendix. 
Code Block \ref{a:runq8} of the Appendix was used to run numerous iterations with varying process counts, and Code Block \ref{a:plotq8} of the Appendix was used to plot the data.
The performance on the KNL system strays slightly from what would be expected, with the sequential code given to us in the assignment always taking longer than the MPI code. 
This is unexpected, as it should take less time than the statically or dynamically scheduled code utilizing 2 processes given there is overhead in the form of MPI initialization and communication.
The expected result can be seen in the locally run code in Figure \ref{fig:q8S}, where the baseline sequential code only performs better when the process count is 2.
Afterwards, the performance is as expected, where the more processes there are, the better the performance is. 
This holds true on both the KNL system and my local machine until 32 processes are reached at which point my system no longer has physical threads to assign to the processes, and as such, performance becomes worse when MPI begins oversubscription.
I am surprised to see that on both systems, the dynamically scheduled code is more performant all the time, I was expecting the 800 \texttt{MPI\_Send/Recv)()} operations used in dynamic scheduling to incur a larger overhead and thus perform worse than the statically assigned tasks, given that regardless of scheduling method, single \texttt{MPI\_Ints} are being sent.
It seems unlikely that this is an analysis error, as for each process count, 100 iterations were conducted.
It is more likely that this can be explained by the extra \texttt{malloc} and \texttt{realloc} operations leveraged by the static code, along with the possibility that different threads are much faster than others.
I would have been interested to see what varied work bundles beyond single rows would have resulted in for the dynamically scheduled code.

\begin{figure*}
\centering
\begin{subfigure}[t]{0.48\textwidth}
  \centering
    \includegraphics[width=\textwidth]{./images/knlq8.png}
  \caption{KNL performance graph}\label{fig:q8S}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{0.48\textwidth}
  \centering
    \includegraphics[width=\textwidth]{./images/localq8.png}
  \caption{Local performance graph}\label{fig:q8D}
\end{subfigure}%
\caption{Graphs depicting difference in performance between sequential (1 process) and statically\/dynamically scheduled MPI code at a variety of process counts (2-32)}
\label{fig:q8}
\end{figure*}


\section*{Q9:}

\subsection*{Code:}
\lstinputlisting[language=C, caption=\centering{MPI code for question 9}, label={q9:mpi}]{../q9/mpi.c}

\subsection*{Log:}
\lstinputlisting[language=sh, caption=\centering{MPI output for question 9}, label={q9:mpilog}]{../q9/mpi.log}

\begin{appendices}\label{Appendix}
  \lstinputlisting[language=make, caption=\centering{Makefile template used for all code}, label={a:make}]{../q1/makefile}
  \lstinputlisting[language=make, caption=\centering{Bash script used to vary process, thread, and array size for question 7}, label={a:runq7}]{../q7/run.sh}
  \lstinputlisting[language=make, caption=\centering{Python script used to plot results for question 7}, label={a:plotq7}]{../q7/results.py}
  \lstinputlisting[language=make, caption=\centering{Bash script used to process counts for question 8}, label={a:runq8}]{../q8/run.sh}
  \lstinputlisting[language=make, caption=\centering{Python script used to plot results for question 8}, label={a:plotq8}]{../q8/results.py}

\end{appendices}

\newpage

% \bibliography{./bibliography/master.bib}
% \bibliographystyle{IEEEtran}

\end{document}
